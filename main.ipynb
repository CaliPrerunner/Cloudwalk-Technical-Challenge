{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Coudwalk Technical Challenge\n",
    "\n",
    "## Objective:\n",
    "Your task is to build a lightweight prototype that listens to spoken digits (0â€“9) and predicts the correct number. The goal is to find the lightest effective solution. Live microphone input to test your model in real time. This helps explore real-world performance, including latency, noise handling, and usability under less controlled conditions."
   ],
   "id": "c0e5800d150b07d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data loading & Preprocessing",
   "id": "45fd2f19f6dce336"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:52:06.053255Z",
     "start_time": "2025-08-14T23:52:04.437874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data loading & Preprocessing for DS-CNN on Free Spoken Digit Dataset (HF: mteb/free-spoken-digit-dataset)\n",
    "# - Loads dataset and derives label names directly from HF features\n",
    "# - Converts audio to Log-Mel spectrograms suitable for DS-CNN: shape [1, n_mels, T]\n",
    "# - Provides modular utilities and a robust collate function\n",
    "# - Exposes: raw (DatasetDict), label_names, id2label, label2id, mapped_splits (DatasetDict), collate\n",
    "\n",
    "# Ensure required packages\n",
    "try:\n",
    "    from datasets import load_dataset, Audio, DatasetDict\n",
    "except ModuleNotFoundError:\n",
    "    !pip install datasets[audio]\n",
    "    from datasets import load_dataset, Audio, DatasetDict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Torchaudio for spectrograms\n",
    "try:\n",
    "    import torchaudio\n",
    "    from torchaudio.transforms import MelSpectrogram\n",
    "    from torchaudio.functional import amplitude_to_DB\n",
    "except ModuleNotFoundError:\n",
    "    !pip install torchaudio\n",
    "    import torchaudio\n",
    "    from torchaudio.transforms import MelSpectrogram\n",
    "    from torchaudio.functional import amplitude_to_DB\n",
    "\n",
    "\n",
    "def get_or_default(name: str, default):\n",
    "    \"\"\"Fetch a global variable by name if it exists; otherwise return default.\"\"\"\n",
    "    return globals()[name] if name in globals() else default\n",
    "\n",
    "\n",
    "# Configuration (use existing globals if already defined elsewhere in the notebook)\n",
    "target_sample_rate: int = get_or_default(\"target_sample_rate\", 16000)\n",
    "n_mels: int = get_or_default(\"n_mels\", 40)\n",
    "n_fft_val: int = get_or_default(\"n_fft_val\", 512)\n",
    "win_length: int = get_or_default(\"win_length\", int(0.025 * target_sample_rate))  # ~25ms\n",
    "hop_length: int = get_or_default(\"hop_length\", int(0.010 * target_sample_rate))  # ~10ms\n",
    "batch_size: int = get_or_default(\"batch_size\", 128)\n",
    "num_workers: int = get_or_default(\"num_workers\", 2)\n",
    "\n",
    "# 1) Load dataset and derive label names (no hardcoding)\n",
    "raw: DatasetDict = load_dataset(\"mteb/free-spoken-digit-dataset\")\n",
    "label_names = raw[\"train\"].features[\"label\"].names\n",
    "id2label = {i: name for i, name in enumerate(label_names)}\n",
    "label2id = {name: i for i, name in id2label.items()}\n",
    "\n",
    "# Cast/Decode audio at target sampling rate\n",
    "raw = raw.cast_column(\"audio\", Audio(sampling_rate=target_sample_rate))\n",
    "\n",
    "# 2) Preprocess: audio -> Log-Mel spectrogram\n",
    "# Reuse existing mel transform if available, else create a new one\n",
    "if \"melspec\" in globals() and isinstance(globals()[\"melspec\"], MelSpectrogram):\n",
    "    melspec: MelSpectrogram = globals()[\"melspec\"]\n",
    "else:\n",
    "    melspec = MelSpectrogram(\n",
    "        sample_rate=target_sample_rate,\n",
    "        n_fft=n_fft_val,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        f_min=0.0,\n",
    "        f_max=target_sample_rate // 2,\n",
    "        n_mels=n_mels,\n",
    "        center=True,\n",
    "        power=2.0,  # power spectrogram\n",
    "        norm=\"slaney\",\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "\n",
    "def waveform_to_logmel(waveform: torch.Tensor, sample_rate: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert mono waveform [T] or [1, T] to Log-Mel spectrogram [1, n_mels, time].\n",
    "    \"\"\"\n",
    "    if waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)  # [1, T]\n",
    "    # Ensure target sample rate; dataset cast handles this, but keep signature consistent\n",
    "    with torch.no_grad():\n",
    "        mel = melspec(waveform)  # [1, n_mels, time]\n",
    "        # Convert to log scale (dB). Add small offset to avoid log(0).\n",
    "        log_mel = amplitude_to_DB(mel.clamp_min(1e-10), multiplier=10.0, amin=1e-10, db_multiplier=0.0)\n",
    "        # Optional per-utterance normalization (stabilizes training for small models)\n",
    "        mean = log_mel.mean(dim=(-1, -2), keepdim=True)\n",
    "        std = log_mel.std(dim=(-1, -2), keepdim=True).clamp_min(1e-5)\n",
    "        log_mel = (log_mel - mean) / std\n",
    "    return log_mel  # [1, n_mels, time]\n",
    "\n",
    "\n",
    "def _map_example_to_features(batch):\n",
    "    \"\"\"\n",
    "    HF map function: takes an example with keys {\"audio\": {\"array\", \"sampling_rate\"}, \"label\"}\n",
    "    Returns dict with \"input_values\": float32 tensor-like [1, n_mels, time] and \"label\": int\n",
    "    \"\"\"\n",
    "    arr = batch[\"audio\"][\"array\"]\n",
    "    sr = batch[\"audio\"][\"sampling_rate\"]\n",
    "    # Convert to torch waveform\n",
    "    wf = torch.tensor(arr, dtype=torch.float32)\n",
    "    # Compute log-mel\n",
    "    features = waveform_to_logmel(wf, sr)\n",
    "    # Store as list to be HF-serializable; collate will convert back to tensor\n",
    "    return {\n",
    "        \"input_values\": features.squeeze(0).numpy()[None, ...].astype(np.float32),  # [1, n_mels, T] as numpy\n",
    "        \"label\": int(batch[\"label\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply mapping to all splits\n",
    "mapped_splits: DatasetDict = raw.map(\n",
    "    _map_example_to_features,\n",
    "    remove_columns=[c for c in raw[\"train\"].column_names if c not in (\"label\",)],\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Collate for DS-CNN: pads along time dimension to max(T) in batch; output [B, 1, n_mels, T]\n",
    "def collate(batch):\n",
    "    \"\"\"\n",
    "    Collate function:\n",
    "    - Accepts items with {\"input_values\": [1, n_mels, T], \"label\": int}\n",
    "    - Right-pads time dimension to the max length in batch\n",
    "    - Returns (features [B, 1, n_mels, T], labels [B])\n",
    "    \"\"\"\n",
    "    feats, labels = [], []\n",
    "    time_dim = -1\n",
    "    for item in batch:\n",
    "        x = item[\"input_values\"]\n",
    "        # Ensure torch tensor [1, n_mels, T]\n",
    "        if isinstance(x, list):\n",
    "            x = np.array(x, dtype=np.float32)\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)  # [1, n_mels, T]\n",
    "        feats.append(x)\n",
    "        labels.append(int(item[\"label\"]))\n",
    "    # Pad along time\n",
    "    max_T = max(x.shape[time_dim] for x in feats)\n",
    "    padded = []\n",
    "    for x in feats:\n",
    "        t = x.shape[time_dim]\n",
    "        if t < max_T:\n",
    "            pad_shape = list(x.shape)\n",
    "            pad_shape[time_dim] = max_T - t\n",
    "            pad = torch.zeros(pad_shape, dtype=x.dtype)\n",
    "            x = torch.cat([x, pad], dim=time_dim)\n",
    "        elif t > max_T:\n",
    "            idx = [slice(None)] * x.dim()\n",
    "            idx[time_dim] = slice(0, max_T)\n",
    "            x = x[tuple(idx)]\n",
    "        padded.append(x)\n",
    "    features = torch.stack(padded, dim=0)  # [B, 1, n_mels, T]\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Expose commonly used globals for downstream cells\n",
    "globals().update({\n",
    "    \"raw\": raw,\n",
    "    \"label_names\": label_names,\n",
    "    \"id2label\": id2label,\n",
    "    \"label2id\": label2id,\n",
    "    \"mapped_splits\": mapped_splits,\n",
    "    \"collate\": collate,\n",
    "    \"target_sample_rate\": target_sample_rate,\n",
    "    \"n_mels\": n_mels,\n",
    "    \"n_fft_val\": n_fft_val,\n",
    "    \"win_length\": win_length,\n",
    "    \"hop_length\": hop_length,\n",
    "    \"melspec\": melspec,\n",
    "})\n"
   ],
   "id": "a1f27adef9e21739",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DS-CNN Model & Training",
   "id": "770b13ae1191784f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:52:06.109281Z",
     "start_time": "2025-08-14T23:52:06.089924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class DSConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise Separable Convolution Block:\n",
    "    - Depthwise 2D convolution (groups=in_channels)\n",
    "    - BatchNorm + ReLU\n",
    "    - Pointwise 1x1 convolution\n",
    "    - BatchNorm + ReLU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: tuple[int, int] = (1, 1),\n",
    "                 dilation: int = 1):\n",
    "        super().__init__()\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                   dilation=dilation, groups=in_channels, bias=False)\n",
    "        self.dw_bn = nn.BatchNorm2d(in_channels)\n",
    "        self.dw_relu = nn.ReLU(inplace=True)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.pw_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.pw_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.depthwise(x)\n",
    "        x = self.dw_bn(x)\n",
    "        x = self.dw_relu(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.pw_bn(x)\n",
    "        x = self.pw_relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DSCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise Separable CNN for keyword spotting / digit recognition.\n",
    "    Expects input of shape [B, 1, n_mels(=40), T].\n",
    "    Architecture:\n",
    "      - Stem conv\n",
    "      - Stack of DSConv blocks with occasional stride for downsampling\n",
    "      - Global average pooling\n",
    "      - Linear classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_mels: int = 40, n_classes: int = 10, channels: tuple[int, ...] = (64, 64, 128, 128, 256)):\n",
    "        super().__init__()\n",
    "        c1, c2, c3, c4, c5 = channels\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.features = nn.Sequential(\n",
    "            DSConvBlock(c1, c2, kernel_size=3, stride=(2, 2)),  # downsample both axes\n",
    "            DSConvBlock(c2, c3, kernel_size=3, stride=(1, 2)),  # further reduce time\n",
    "            DSConvBlock(c3, c4, kernel_size=3, stride=(2, 1)),  # reduce mel\n",
    "            DSConvBlock(c4, c5, kernel_size=3, stride=(1, 1)),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(c5, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.stem(x)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "id": "d4716b53a26e6a48",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:52:06.217460Z",
     "start_time": "2025-08-14T23:52:06.154683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "def _build_splits_from_prepared(prepared: dict, val_size: float = 0.1) -> dict:\n",
    "    \"\"\"\n",
    "    Create train/validation/test splits from prepared_splits.\n",
    "    If validation not present, split from train with stratification.\n",
    "    \"\"\"\n",
    "    splits = dict(prepared)\n",
    "    if \"validation\" not in splits:\n",
    "        tv = splits[\"train\"].train_test_split(test_size=val_size, stratify_by_column=\"label\")\n",
    "        splits[\"train\"], splits[\"validation\"] = tv[\"train\"], tv[\"test\"]\n",
    "    return splits\n",
    "\n",
    "\n",
    "def _num_classes_from_dataset(ds) -> int:\n",
    "    labs = list(set(ds[\"label\"]))\n",
    "    return int(max(labs) + 1)\n",
    "\n",
    "\n",
    "def build_dataloaders(splits: dict, collate_fn, batch_size: int = 128, num_workers: int = 2) -> tuple[\n",
    "    DataLoader, DataLoader, DataLoader]:\n",
    "    train_loader = DataLoader(splits[\"train\"], batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                              collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(splits[\"validation\"], batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                            collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(splits[\"test\"], batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                             collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "prepared_with_val = _build_splits_from_prepared(mapped_splits, val_size=0.1)\n",
    "n_classes = _num_classes_from_dataset(prepared_with_val[\"train\"])\n",
    "train_loader, val_loader, test_loader = build_dataloaders(prepared_with_val, collate, batch_size=128,\n",
    "                                                          num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DSCNN(n_mels=40, n_classes=n_classes).to(device)\n",
    "param_count = count_parameters(model)\n",
    "\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_ckpt_path = models_dir / \"dscnn_best2.pt\"\n"
   ],
   "id": "6d69701fc9cc4bf5",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Loop",
   "id": "c8aef2575273a35a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:52:06.270471Z",
     "start_time": "2025-08-14T23:52:06.229740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from typing import Dict, List, Tuple, Callable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _macro_micro_from_confusion(conf_mat: torch.Tensor) -> Dict[str, float]:\n",
    "    # conf_mat shape: [C, C] where rows = true, cols = pred\n",
    "    cm = conf_mat.float()\n",
    "    tp = torch.diag(cm)\n",
    "    fp = cm.sum(0) - tp\n",
    "    fn = cm.sum(1) - tp\n",
    "    tn = cm.sum() - (tp + fp + fn)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    eps = 1e-12\n",
    "\n",
    "    precision_per_class = tp / torch.clamp(tp + fp, min=eps)\n",
    "    recall_per_class = tp / torch.clamp(tp + fn, min=eps)\n",
    "    f1_per_class = 2 * precision_per_class * recall_per_class / torch.clamp(precision_per_class + recall_per_class,\n",
    "                                                                            min=eps)\n",
    "\n",
    "    macro_precision = precision_per_class.mean().item()\n",
    "    macro_recall = recall_per_class.mean().item()\n",
    "    macro_f1 = f1_per_class.mean().item()\n",
    "\n",
    "    # Micro = compute from totals\n",
    "    tp_sum = tp.sum()\n",
    "    fp_sum = fp.sum()\n",
    "    fn_sum = fn.sum()\n",
    "    micro_precision = (tp_sum / torch.clamp(tp_sum + fp_sum, min=eps)).item()\n",
    "    micro_recall = (tp_sum / torch.clamp(tp_sum + fn_sum, min=eps)).item()\n",
    "    micro_f1 = (2 * micro_precision * micro_recall / max(1e-12, (micro_precision + micro_recall))) if (\n",
    "                                                                                                              micro_precision + micro_recall) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision_macro\": macro_precision,\n",
    "        \"recall_macro\": macro_recall,\n",
    "        \"f1_macro\": macro_f1,\n",
    "        \"precision_micro\": micro_precision,\n",
    "        \"recall_micro\": micro_recall,\n",
    "        \"f1_micro\": micro_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_confusion(model: nn.Module, loader: DataLoader, device: torch.device, n_classes: int) -> Dict[\n",
    "    str, float]:\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    conf_mat = torch.zeros((n_classes, n_classes), dtype=torch.long, device=device)\n",
    "\n",
    "    for feats, labels in loader:\n",
    "        feats = feats.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = model(feats)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "\n",
    "        # Update confusion matrix\n",
    "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "            conf_mat[t.long(), p.long()] += 1\n",
    "\n",
    "    metrics = _macro_micro_from_confusion(conf_mat)\n",
    "    metrics.update({\n",
    "        \"loss\": loss_sum / max(1, total),\n",
    "        \"acc\": correct / max(1, total),\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_one_epoch_with_metrics(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer,\n",
    "                                 device: torch.device, n_classes: int,\n",
    "                                 scaler: Optional[torch.cuda.amp.GradScaler] = None) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    correct = 0\n",
    "    conf_mat = torch.zeros((n_classes, n_classes), dtype=torch.long, device=device)\n",
    "\n",
    "    # Gradient clipping config\n",
    "    max_norm = 1.0  # L2 norm clipping\n",
    "\n",
    "    for b_idx, (feats, labels) in enumerate(loader):\n",
    "        feats = feats.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(feats)\n",
    "                loss = criterion(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Unscale then clip before stepping\n",
    "            scaler.unscale_(optimizer)\n",
    "            global_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(feats)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip before optimizer.step()\n",
    "            global_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "            conf_mat[t.long(), p.long()] += 1\n",
    "\n",
    "    base = {\n",
    "        \"loss\": loss_sum / max(1, total),\n",
    "        \"acc\": correct / max(1, total),\n",
    "    }\n",
    "    base.update(_macro_micro_from_confusion(conf_mat))\n",
    "    return base\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_model_only_latency(model: nn.Module, loader: DataLoader, device: torch.device,\n",
    "                               warmup_batches: int = 5, max_batches: Optional[int] = None) -> Dict[str, float]:\n",
    "    # Measures forward-pass latency from already-precomputed features to logits.\n",
    "    model.eval()\n",
    "    times: List[float] = []\n",
    "    seen = 0\n",
    "\n",
    "    # Warmup\n",
    "    it = iter(loader)\n",
    "    for _ in range(warmup_batches):\n",
    "        try:\n",
    "            feats, _ = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        _ = model(feats.to(device))\n",
    "\n",
    "    # Timed runs\n",
    "    it = iter(loader)\n",
    "    with torch.inference_mode():\n",
    "        for b_idx, (feats, _) in enumerate(it):\n",
    "            if max_batches is not None and b_idx >= max_batches:\n",
    "                break\n",
    "            feats = feats.to(device)\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            t0 = time.perf_counter()\n",
    "            _ = model(feats)\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            t1 = time.perf_counter()\n",
    "            batch_time = t1 - t0\n",
    "            per_sample = batch_time / max(1, feats.size(0))\n",
    "            times.append(per_sample)\n",
    "            seen += feats.size(0)\n",
    "\n",
    "    if len(times) == 0:\n",
    "        return {\"latency_mean_s\": float(\"nan\"), \"latency_p50_s\": float(\"nan\"), \"latency_p95_s\": float(\"nan\")}\n",
    "    arr = np.array(times)\n",
    "    return {\n",
    "        \"latency_mean_s\": float(arr.mean()),\n",
    "        \"latency_p50_s\": float(np.percentile(arr, 50)),\n",
    "        \"latency_p95_s\": float(np.percentile(arr, 95)),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_end_to_end_latency(\n",
    "        model: nn.Module,\n",
    "        dataset,\n",
    "        device: torch.device,\n",
    "        featurize_fn: Callable[[torch.Tensor, int], torch.Tensor],\n",
    "        sample_rate_getter: Optional[Callable[[int], int]] = None,\n",
    "        num_samples: int = 100,\n",
    "        warmup: int = 10,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Measures end-to-end latency from raw audio -> features -> classification.\n",
    "\n",
    "    Arguments:\n",
    "      - dataset: indexable dataset where dataset[i] returns either (waveform, label) or (waveform, sample_rate, label)\n",
    "      - featurize_fn: function that maps (waveform, sample_rate) -> features tensor shaped as model expects (C, T, ...) or (F, T)\n",
    "      - sample_rate_getter: optional function to get sample rate when dataset[i] does not return it; called as sample_rate_getter(i)\n",
    "      - num_samples: number of random samples from dataset to time\n",
    "      - warmup: number of warmup runs to exclude from timing\n",
    "\n",
    "    Returns dict with mean/p50/p95 in seconds per sample.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rng = np.random.default_rng(0)\n",
    "    indices = rng.choice(len(dataset), size=min(num_samples + warmup, len(dataset)), replace=False)\n",
    "    times: List[float] = []\n",
    "\n",
    "    # Helper to extract (waveform, sr) regardless of dataset format\n",
    "    def _get_waveform_sr(idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        item = dataset[idx]\n",
    "        if isinstance(item, (tuple, list)):\n",
    "            if len(item) == 3:\n",
    "                waveform, sample_rate, _ = item\n",
    "            elif len(item) == 2:\n",
    "                waveform, _ = item\n",
    "                sample_rate = sample_rate_getter(idx) if sample_rate_getter is not None else 16000\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported dataset item format.\")\n",
    "        else:\n",
    "            raise ValueError(\"Dataset item must be tuple/list.\")\n",
    "        return waveform, int(sample_rate)\n",
    "\n",
    "    # Warmup\n",
    "    for i in range(min(warmup, len(indices))):\n",
    "        wf, sr = _get_waveform_sr(indices[i])\n",
    "        feats = featurize_fn(wf, sr)\n",
    "        feats = feats.unsqueeze(0).to(device) if feats.dim() == 3 else feats.to(device).unsqueeze(0)\n",
    "        _ = model(feats)\n",
    "\n",
    "    # Timed runs\n",
    "    with torch.inference_mode():\n",
    "        for i in range(warmup, len(indices)):\n",
    "            wf, sr = _get_waveform_sr(indices[i])\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            t0 = time.perf_counter()\n",
    "            feats = featurize_fn(wf, sr)\n",
    "            feats = feats.unsqueeze(0).to(device) if feats.dim() == 3 else feats.to(device).unsqueeze(0)\n",
    "            _ = model(feats)\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            t1 = time.perf_counter()\n",
    "            times.append(t1 - t0)\n",
    "\n",
    "    if len(times) == 0:\n",
    "        return {\"latency_mean_s\": float(\"nan\"), \"latency_p50_s\": float(\"nan\"), \"latency_p95_s\": float(\"nan\")}\n",
    "    arr = np.array(times)\n",
    "    return {\n",
    "        \"latency_mean_s\": float(arr.mean()),\n",
    "        \"latency_p50_s\": float(np.percentile(arr, 50)),\n",
    "        \"latency_p95_s\": float(np.percentile(arr, 95)),\n",
    "        \"samples_timed\": int(len(arr)),\n",
    "    }\n"
   ],
   "id": "5511742d82087992",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:12:02.139092Z",
     "start_time": "2025-08-14T23:52:06.294958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assumes you already have:\n",
    "# - model, train_loader, val_loader, test_loader, device, n_classes, best_ckpt_path\n",
    "# - epochs, optimizer, scheduler, scaler (optional)\n",
    "# If you previously defined train_one_epoch/evaluate, we now use the enhanced versions below.\n",
    "\n",
    "# Robust collate to handle cases where \"input_values\" may be a list instead of a Tensor\n",
    "def _safe_collate(batch):\n",
    "    import torch\n",
    "    feats = []\n",
    "    labels = []\n",
    "    time_dim = -1\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        if isinstance(x, list):\n",
    "            # unwrap single-element lists\n",
    "            x = x[0] if len(x) > 0 else x\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x)\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)  # [1, n_mels, T]\n",
    "        feats.append(x)\n",
    "        labels.append(int(b[\"label\"]))\n",
    "    # pad/truncate to max time in batch\n",
    "    max_time = max(x.shape[time_dim] for x in feats)\n",
    "    padded = []\n",
    "    for x in feats:\n",
    "        t = x.shape[time_dim]\n",
    "        if t > max_time:\n",
    "            idx = [slice(None)] * x.dim()\n",
    "            idx[time_dim] = slice(0, max_time)\n",
    "            x = x[tuple(idx)]\n",
    "        elif t < max_time:\n",
    "            pad_shape = list(x.shape)\n",
    "            pad_shape[time_dim] = max_time - t\n",
    "            pad_tensor = torch.zeros(pad_shape, dtype=x.dtype)\n",
    "            x = torch.cat([x, pad_tensor], dim=time_dim)\n",
    "        padded.append(x)\n",
    "    batch_features = torch.stack(padded, dim=0)  # [B, 1, n_mels, time]\n",
    "    batch_labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "\n",
    "# Rebuild DataLoaders with the robust collate to avoid shape/list issues\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    prepared_with_val[\"train\"],\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=_safe_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    prepared_with_val[\"validation\"],\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=_safe_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    prepared_with_val[\"test\"],\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=_safe_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "\n",
    "# Clear session and CUDA cache between runs, then re-instantiate everything fresh\n",
    "def clear_session():\n",
    "    import gc\n",
    "    import torch\n",
    "    # Drop references if they exist\n",
    "    for name in (\"model\", \"optimizer\", \"scheduler\", \"scaler\"):\n",
    "        if name in globals():\n",
    "            try:\n",
    "                del globals()[name]\n",
    "            except Exception:\n",
    "                pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "clear_session()\n",
    "\n",
    "torch.manual_seed(1212)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(1212)\n",
    "\n",
    "# Fresh instantiation for each run\n",
    "epochs = 80\n",
    "lr = 0.0002\n",
    "\n",
    "model = DSCNN(n_mels=40, n_classes=n_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "best_val_acc = 0.0\n",
    "history = {\n",
    "    \"train_loss\": [], \"train_acc\": [],\n",
    "    \"train_precision_macro\": [], \"train_recall_macro\": [], \"train_f1_macro\": [],\n",
    "    \"val_loss\": [], \"val_acc\": [],\n",
    "    \"val_precision_macro\": [], \"val_recall_macro\": [], \"val_f1_macro\": [],\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()  # ensure training mode each epoch\n",
    "    train_metrics = train_one_epoch_with_metrics(model, train_loader, optimizer, device, n_classes, scaler)\n",
    "    val_metrics = evaluate_with_confusion(model, val_loader, device, n_classes)\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "    history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "    history[\"train_precision_macro\"].append(train_metrics[\"precision_macro\"])\n",
    "    history[\"train_recall_macro\"].append(train_metrics[\"recall_macro\"])\n",
    "    history[\"train_f1_macro\"].append(train_metrics[\"f1_macro\"])\n",
    "\n",
    "    history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "    history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "    history[\"val_precision_macro\"].append(val_metrics[\"precision_macro\"])\n",
    "    history[\"val_recall_macro\"].append(val_metrics[\"recall_macro\"])\n",
    "    history[\"val_f1_macro\"].append(val_metrics[\"f1_macro\"])\n",
    "\n",
    "    if val_metrics[\"acc\"] > best_val_acc:\n",
    "        best_val_acc = val_metrics[\"acc\"]\n",
    "        torch.save(\n",
    "            {\"model_state\": model.state_dict(), \"config\": {\"n_classes\": n_classes}},\n",
    "            best_ckpt_path\n",
    "        )\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train: loss={train_metrics['loss']:.4f}, acc={train_metrics['acc']:.4f}, f1={train_metrics['f1_macro']:.4f} | \"\n",
    "          f\"Val: loss={val_metrics['loss']:.4f}, acc={val_metrics['acc']:.4f}, f1={val_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "best_val_acc\n"
   ],
   "id": "19683b84b452fa6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train: loss=2.3112, acc=0.1148, f1=0.0445 | Val: loss=2.3064, acc=0.1000, f1=0.0182\n",
      "Epoch 02 | Train: loss=2.2522, acc=0.2214, f1=0.1428 | Val: loss=2.2844, acc=0.1185, f1=0.0487\n",
      "Epoch 03 | Train: loss=2.2087, acc=0.2988, f1=0.2776 | Val: loss=2.2017, acc=0.2519, f1=0.1953\n",
      "Epoch 04 | Train: loss=2.1463, acc=0.3074, f1=0.2752 | Val: loss=2.1403, acc=0.2185, f1=0.1825\n",
      "Epoch 05 | Train: loss=2.0817, acc=0.3346, f1=0.2947 | Val: loss=2.0706, acc=0.2963, f1=0.2491\n",
      "Epoch 06 | Train: loss=2.0131, acc=0.3654, f1=0.3221 | Val: loss=2.0033, acc=0.3222, f1=0.2616\n",
      "Epoch 07 | Train: loss=1.9482, acc=0.3959, f1=0.3523 | Val: loss=1.9203, acc=0.3704, f1=0.3217\n",
      "Epoch 08 | Train: loss=1.8644, acc=0.4580, f1=0.4274 | Val: loss=1.8508, acc=0.4481, f1=0.4167\n",
      "Epoch 09 | Train: loss=1.7868, acc=0.5370, f1=0.5137 | Val: loss=1.7711, acc=0.5037, f1=0.4658\n",
      "Epoch 10 | Train: loss=1.7121, acc=0.5786, f1=0.5502 | Val: loss=1.6727, acc=0.5296, f1=0.4896\n",
      "Epoch 11 | Train: loss=1.6115, acc=0.6300, f1=0.6071 | Val: loss=1.5871, acc=0.5889, f1=0.5657\n",
      "Epoch 12 | Train: loss=1.5256, acc=0.6860, f1=0.6628 | Val: loss=1.5047, acc=0.6815, f1=0.6610\n",
      "Epoch 13 | Train: loss=1.4371, acc=0.7305, f1=0.7152 | Val: loss=1.4094, acc=0.7259, f1=0.7159\n",
      "Epoch 14 | Train: loss=1.3455, acc=0.7576, f1=0.7387 | Val: loss=1.3274, acc=0.7259, f1=0.7089\n",
      "Epoch 15 | Train: loss=1.2784, acc=0.7905, f1=0.7762 | Val: loss=1.2446, acc=0.7667, f1=0.7547\n",
      "Epoch 16 | Train: loss=1.1987, acc=0.8251, f1=0.8167 | Val: loss=1.1581, acc=0.8000, f1=0.7899\n",
      "Epoch 17 | Train: loss=1.1263, acc=0.8436, f1=0.8362 | Val: loss=1.1052, acc=0.8222, f1=0.8169\n",
      "Epoch 18 | Train: loss=1.0630, acc=0.8568, f1=0.8496 | Val: loss=1.0413, acc=0.7963, f1=0.7840\n",
      "Epoch 19 | Train: loss=0.9985, acc=0.8733, f1=0.8683 | Val: loss=0.9694, acc=0.8556, f1=0.8521\n",
      "Epoch 20 | Train: loss=0.9414, acc=0.8860, f1=0.8813 | Val: loss=0.9136, acc=0.8556, f1=0.8517\n",
      "Epoch 21 | Train: loss=0.8782, acc=0.9033, f1=0.8993 | Val: loss=0.8476, acc=0.8741, f1=0.8713\n",
      "Epoch 22 | Train: loss=0.8148, acc=0.9181, f1=0.9161 | Val: loss=0.8399, acc=0.8704, f1=0.8667\n",
      "Epoch 23 | Train: loss=0.7713, acc=0.9230, f1=0.9211 | Val: loss=0.7776, acc=0.8852, f1=0.8830\n",
      "Epoch 24 | Train: loss=0.7373, acc=0.9329, f1=0.9313 | Val: loss=0.7222, acc=0.8778, f1=0.8754\n",
      "Epoch 25 | Train: loss=0.6939, acc=0.9337, f1=0.9325 | Val: loss=0.6981, acc=0.8926, f1=0.8909\n",
      "Epoch 26 | Train: loss=0.6396, acc=0.9461, f1=0.9448 | Val: loss=0.6675, acc=0.9074, f1=0.9063\n",
      "Epoch 27 | Train: loss=0.6212, acc=0.9416, f1=0.9405 | Val: loss=0.6276, acc=0.9222, f1=0.9219\n",
      "Epoch 28 | Train: loss=0.5893, acc=0.9486, f1=0.9478 | Val: loss=0.6057, acc=0.9333, f1=0.9332\n",
      "Epoch 29 | Train: loss=0.5690, acc=0.9527, f1=0.9522 | Val: loss=0.5609, acc=0.9370, f1=0.9362\n",
      "Epoch 30 | Train: loss=0.5502, acc=0.9556, f1=0.9548 | Val: loss=0.5400, acc=0.9259, f1=0.9259\n",
      "Epoch 31 | Train: loss=0.5069, acc=0.9634, f1=0.9629 | Val: loss=0.5188, acc=0.9370, f1=0.9366\n",
      "Epoch 32 | Train: loss=0.5001, acc=0.9642, f1=0.9638 | Val: loss=0.4781, acc=0.9407, f1=0.9407\n",
      "Epoch 33 | Train: loss=0.4620, acc=0.9658, f1=0.9655 | Val: loss=0.4850, acc=0.9556, f1=0.9549\n",
      "Epoch 34 | Train: loss=0.4443, acc=0.9700, f1=0.9697 | Val: loss=0.4823, acc=0.9481, f1=0.9476\n",
      "Epoch 35 | Train: loss=0.4220, acc=0.9720, f1=0.9719 | Val: loss=0.4523, acc=0.9593, f1=0.9592\n",
      "Epoch 36 | Train: loss=0.4126, acc=0.9733, f1=0.9731 | Val: loss=0.4448, acc=0.9481, f1=0.9476\n",
      "Epoch 37 | Train: loss=0.4100, acc=0.9745, f1=0.9743 | Val: loss=0.4051, acc=0.9556, f1=0.9554\n",
      "Epoch 38 | Train: loss=0.3749, acc=0.9761, f1=0.9760 | Val: loss=0.4127, acc=0.9630, f1=0.9625\n",
      "Epoch 39 | Train: loss=0.3820, acc=0.9761, f1=0.9759 | Val: loss=0.3914, acc=0.9630, f1=0.9629\n",
      "Epoch 40 | Train: loss=0.3641, acc=0.9765, f1=0.9764 | Val: loss=0.3816, acc=0.9556, f1=0.9550\n",
      "Epoch 41 | Train: loss=0.3458, acc=0.9786, f1=0.9785 | Val: loss=0.3624, acc=0.9667, f1=0.9664\n",
      "Epoch 42 | Train: loss=0.3364, acc=0.9802, f1=0.9801 | Val: loss=0.3739, acc=0.9667, f1=0.9664\n",
      "Epoch 43 | Train: loss=0.3333, acc=0.9815, f1=0.9814 | Val: loss=0.3545, acc=0.9704, f1=0.9700\n",
      "Epoch 44 | Train: loss=0.3234, acc=0.9811, f1=0.9810 | Val: loss=0.3329, acc=0.9704, f1=0.9700\n",
      "Epoch 45 | Train: loss=0.3145, acc=0.9819, f1=0.9818 | Val: loss=0.3189, acc=0.9667, f1=0.9661\n",
      "Epoch 46 | Train: loss=0.3050, acc=0.9852, f1=0.9851 | Val: loss=0.3114, acc=0.9704, f1=0.9700\n",
      "Epoch 47 | Train: loss=0.2872, acc=0.9827, f1=0.9827 | Val: loss=0.3203, acc=0.9667, f1=0.9661\n",
      "Epoch 48 | Train: loss=0.2892, acc=0.9844, f1=0.9843 | Val: loss=0.3051, acc=0.9704, f1=0.9700\n",
      "Epoch 49 | Train: loss=0.2749, acc=0.9885, f1=0.9885 | Val: loss=0.3202, acc=0.9704, f1=0.9700\n",
      "Epoch 50 | Train: loss=0.2561, acc=0.9885, f1=0.9885 | Val: loss=0.3122, acc=0.9704, f1=0.9700\n",
      "Epoch 51 | Train: loss=0.2642, acc=0.9889, f1=0.9889 | Val: loss=0.3040, acc=0.9704, f1=0.9700\n",
      "Epoch 52 | Train: loss=0.2615, acc=0.9889, f1=0.9889 | Val: loss=0.2869, acc=0.9704, f1=0.9700\n",
      "Epoch 53 | Train: loss=0.2625, acc=0.9893, f1=0.9893 | Val: loss=0.2832, acc=0.9704, f1=0.9700\n",
      "Epoch 54 | Train: loss=0.2580, acc=0.9901, f1=0.9901 | Val: loss=0.2733, acc=0.9704, f1=0.9700\n",
      "Epoch 55 | Train: loss=0.2456, acc=0.9914, f1=0.9914 | Val: loss=0.2658, acc=0.9704, f1=0.9700\n",
      "Epoch 56 | Train: loss=0.2449, acc=0.9926, f1=0.9926 | Val: loss=0.2599, acc=0.9704, f1=0.9700\n",
      "Epoch 57 | Train: loss=0.2400, acc=0.9918, f1=0.9918 | Val: loss=0.2475, acc=0.9741, f1=0.9739\n",
      "Epoch 58 | Train: loss=0.2389, acc=0.9922, f1=0.9922 | Val: loss=0.2577, acc=0.9741, f1=0.9739\n",
      "Epoch 59 | Train: loss=0.2382, acc=0.9914, f1=0.9914 | Val: loss=0.2526, acc=0.9741, f1=0.9739\n",
      "Epoch 60 | Train: loss=0.2363, acc=0.9918, f1=0.9918 | Val: loss=0.2609, acc=0.9741, f1=0.9739\n",
      "Epoch 61 | Train: loss=0.2266, acc=0.9926, f1=0.9926 | Val: loss=0.2552, acc=0.9704, f1=0.9700\n",
      "Epoch 62 | Train: loss=0.2274, acc=0.9922, f1=0.9922 | Val: loss=0.2483, acc=0.9741, f1=0.9739\n",
      "Epoch 63 | Train: loss=0.2308, acc=0.9909, f1=0.9909 | Val: loss=0.2471, acc=0.9741, f1=0.9739\n",
      "Epoch 64 | Train: loss=0.2253, acc=0.9930, f1=0.9930 | Val: loss=0.2400, acc=0.9741, f1=0.9739\n",
      "Epoch 65 | Train: loss=0.2249, acc=0.9934, f1=0.9934 | Val: loss=0.2418, acc=0.9741, f1=0.9739\n",
      "Epoch 66 | Train: loss=0.2190, acc=0.9938, f1=0.9938 | Val: loss=0.2421, acc=0.9741, f1=0.9739\n",
      "Epoch 67 | Train: loss=0.2177, acc=0.9938, f1=0.9938 | Val: loss=0.2336, acc=0.9741, f1=0.9739\n",
      "Epoch 68 | Train: loss=0.2172, acc=0.9938, f1=0.9938 | Val: loss=0.2352, acc=0.9741, f1=0.9739\n",
      "Epoch 69 | Train: loss=0.2218, acc=0.9930, f1=0.9930 | Val: loss=0.2232, acc=0.9741, f1=0.9739\n",
      "Epoch 70 | Train: loss=0.2197, acc=0.9947, f1=0.9947 | Val: loss=0.2458, acc=0.9741, f1=0.9739\n",
      "Epoch 71 | Train: loss=0.2178, acc=0.9942, f1=0.9942 | Val: loss=0.2362, acc=0.9741, f1=0.9739\n",
      "Epoch 72 | Train: loss=0.2094, acc=0.9926, f1=0.9926 | Val: loss=0.2441, acc=0.9778, f1=0.9776\n",
      "Epoch 73 | Train: loss=0.2155, acc=0.9938, f1=0.9938 | Val: loss=0.2497, acc=0.9778, f1=0.9776\n",
      "Epoch 74 | Train: loss=0.2089, acc=0.9922, f1=0.9922 | Val: loss=0.2475, acc=0.9741, f1=0.9739\n",
      "Epoch 75 | Train: loss=0.2159, acc=0.9942, f1=0.9942 | Val: loss=0.2413, acc=0.9741, f1=0.9739\n",
      "Epoch 76 | Train: loss=0.2160, acc=0.9938, f1=0.9938 | Val: loss=0.2358, acc=0.9741, f1=0.9739\n",
      "Epoch 77 | Train: loss=0.2157, acc=0.9930, f1=0.9930 | Val: loss=0.2453, acc=0.9741, f1=0.9739\n",
      "Epoch 78 | Train: loss=0.2109, acc=0.9942, f1=0.9942 | Val: loss=0.2475, acc=0.9778, f1=0.9776\n",
      "Epoch 79 | Train: loss=0.2133, acc=0.9930, f1=0.9930 | Val: loss=0.2346, acc=0.9741, f1=0.9739\n",
      "Epoch 80 | Train: loss=0.2091, acc=0.9938, f1=0.9938 | Val: loss=0.2500, acc=0.9778, f1=0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Graphs\n",
   "id": "294802c335958cff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:12:07.377031Z",
     "start_time": "2025-08-15T00:12:02.337537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(\"Plot results\", exist_ok=True)\n",
    "\n",
    "epochs_range = range(1, len(history[\"train_acc\"]) + 1)\n",
    "\n",
    "# 1) Final test metrics\n",
    "with torch.no_grad():\n",
    "    test_metrics = evaluate_with_confusion(model, test_loader, device, n_classes)\n",
    "\n",
    "# 2) Model-only latency (forward pass on precomputed features)\n",
    "#    Limit batches to keep timing quick; adjust as needed.\n",
    "model_latency = measure_model_only_latency(\n",
    "    model, val_loader, device, warmup_batches=5, max_batches=20\n",
    ")\n",
    "\n",
    "# 3) End-to-end latency (raw audio -> features -> model).\n",
    "#    If you have a raw-audio dataset and a featurize_fn available, compute it here.\n",
    "#    For now, set to None so plotting code can skip it gracefully.\n",
    "end_to_end_latency = None\n",
    "\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "print(\"Model-only latency:\", model_latency)\n",
    "\n",
    "# 1) Accuracy curves (own graph)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_range, history[\"train_acc\"], label=\"Train Acc\", marker=\"o\", linewidth=2)\n",
    "plt.plot(epochs_range, history[\"val_acc\"], label=\"Val Acc\", marker=\"s\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Accuracy over epochs\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"Plot results\", \"accuracy_over_epochs.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 2) Precision (macro) curves (own graph)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_range, history[\"train_precision_macro\"], label=\"Train Precision (macro)\", marker=\"o\", linewidth=2)\n",
    "plt.plot(epochs_range, history[\"val_precision_macro\"], label=\"Val Precision (macro)\", marker=\"s\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Precision (macro)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Precision (macro) over epochs\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"Plot results\", \"precision_macro_over_epochs.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 3) F1 (macro) curves (own graph)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_range, history[\"train_f1_macro\"], label=\"Train F1 (macro)\", marker=\"o\", linewidth=2)\n",
    "plt.plot(epochs_range, history[\"val_f1_macro\"], label=\"Val F1 (macro)\", marker=\"s\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 (macro)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"F1 (macro) over epochs\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"Plot results\", \"f1_macro_over_epochs.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 4) Final test set metrics bar chart\n",
    "final_names = [\"Accuracy\", \"Precision (macro)\", \"Recall (macro)\", \"F1 (macro)\"]\n",
    "final_vals = [\n",
    "    test_metrics.get(\"acc\", float(\"nan\")),\n",
    "    test_metrics.get(\"precision_macro\", float(\"nan\")),\n",
    "    test_metrics.get(\"recall_macro\", float(\"nan\")),\n",
    "    test_metrics.get(\"f1_macro\", float(\"nan\")),\n",
    "]\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(final_names, final_vals, color=[\"#4caf50\", \"#2196f3\", \"#ff9800\", \"#9c27b0\"])\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Final Test Metrics\")\n",
    "for b, v in zip(bars, final_vals):\n",
    "    plt.text(b.get_x() + b.get_width() / 2, b.get_height() + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"Plot results\", \"final_test_metrics.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# 5) Latency plot (own graph; model-only; end-to-end if available)\n",
    "# We plot horizontal bars for summary stats in milliseconds for readability\n",
    "def _plot_latency_summary(lat_dict: Dict[str, float], title: str, filename: str):\n",
    "    if lat_dict is None:\n",
    "        return\n",
    "    keys = [k for k in [\"latency_mean_s\", \"latency_p50_s\", \"latency_p95_s\"] if k in lat_dict]\n",
    "    vals_ms = [lat_dict[k] * 1000.0 for k in keys]\n",
    "    labels = [\"Mean (ms)\", \"P50 (ms)\", \"P95 (ms)\"]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bars = plt.barh(labels, vals_ms, color=\"#607d8b\")\n",
    "    plt.xlabel(\"Milliseconds per sample\")\n",
    "    plt.title(title)\n",
    "    for b, v in zip(bars, vals_ms):\n",
    "        plt.text(v, b.get_y() + b.get_height() / 2, f\" {v:.2f} ms\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"Plot results\", filename), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "_plot_latency_summary(model_latency, \"Model-only Latency Summary\", \"latency_model_only.png\")\n",
    "if end_to_end_latency is not None:\n",
    "    _plot_latency_summary(end_to_end_latency, \"End-to-End Latency Summary\", \"latency_end_to_end.png\")\n",
    "\n",
    "print('Saved plots to \"Plot results\" folder.')\n"
   ],
   "id": "96c7a2a443ad8274",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'precision_macro': 0.9838575124740601, 'recall_macro': 0.9833332896232605, 'f1_macro': 0.9833248853683472, 'precision_micro': 0.9833333492279053, 'recall_micro': 0.9833333492279053, 'f1_micro': 0.9833333492279053, 'loss': 0.21504266202449798, 'acc': 0.9833333333333333}\n",
      "Model-only latency: {'latency_mean_s': 0.00212923571013827, 'latency_p50_s': 0.001924420570304619, 'latency_p95_s': 0.0026869930783306015}\n",
      "Saved plots to \"Plot results\" folder.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<llm-snippet-file>/home/bratty_brat/PycharmProjects/CloudwalkTechnical Challenge/main.ipynb</llm-snippet-file>\n",
   "id": "cdfaed31a0f0aa0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Live Microphone Inference (Ubuntu)\n",
   "id": "6d15a74307aa7c63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:15:25.555436Z",
     "start_time": "2025-08-15T00:15:22.618418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Live microphone digit recognition using the trained DS-CNN model (FFmpeg-only).\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "import tempfile\n",
    "import wave\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def _get_or_default(name: str, default):\n",
    "    return globals()[name] if name in globals() else default\n",
    "\n",
    "\n",
    "target_sample_rate = _get_or_default(\"target_sample_rate\", 16000)\n",
    "n_mels = _get_or_default(\"n_mels\", 40)\n",
    "n_fft_val = _get_or_default(\"n_fft_val\", 512)\n",
    "win_length = _get_or_default(\"win_length\", int(0.025 * target_sample_rate))\n",
    "hop_length = _get_or_default(\"hop_length\", int(0.010 * target_sample_rate))\n",
    "label_names = _get_or_default(\"label_names\", [str(i) for i in range(10)])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load trained weights using existing DSCNN definition\n",
    "ckpt_path = Path(\"models\") / \"dscnn_best2.pt\"\n",
    "if not ckpt_path.exists():\n",
    "    alt = Path(\"/models/dscnn_best2.pt\")\n",
    "    ckpt_path = alt if alt.exists() else ckpt_path\n",
    "if not ckpt_path.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {ckpt_path}. Train the model or place the .pt file accordingly.\")\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "n_classes = int(ckpt.get(\"config\", {}).get(\"n_classes\", len(label_names)))\n",
    "model = DSCNN(n_mels=n_mels, n_classes=n_classes).to(device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def _has_ffmpeg() -> bool:\n",
    "    return shutil.which(\"ffmpeg\") is not None\n",
    "\n",
    "\n",
    "def _ffmpeg_input_args(sr: int) -> list[str]:\n",
    "    plat = sys.platform\n",
    "    if plat.startswith(\"linux\"):\n",
    "        return [\"-f\", \"alsa\", \"-i\", \"default\", \"-ac\", \"1\", \"-ar\", str(sr)]\n",
    "    elif plat == \"darwin\":\n",
    "        return [\"-f\", \"avfoundation\", \"-i\", \":0\", \"-ac\", \"1\", \"-ar\", str(sr)]\n",
    "    elif plat.startswith(\"win\"):\n",
    "        return [\"-f\", \"dshow\", \"-i\", \"audio=default\", \"-ac\", \"1\", \"-ar\", str(sr)]\n",
    "    else:\n",
    "        return [\"-f\", \"alsa\", \"-i\", \"default\", \"-ac\", \"1\", \"-ar\", str(sr)]\n",
    "\n",
    "\n",
    "def _read_wav_to_numpy(path: str) -> np.ndarray:\n",
    "    with wave.open(path, \"rb\") as wf:\n",
    "        n_channels = wf.getnchannels()\n",
    "        sampwidth = wf.getsampwidth()\n",
    "        n_frames = wf.getnframes()\n",
    "        raw = wf.readframes(n_frames)\n",
    "\n",
    "    if sampwidth == 2:\n",
    "        dtype = np.int16\n",
    "        scale = 32768.0\n",
    "    elif sampwidth == 4:\n",
    "        dtype = np.int32\n",
    "        scale = 2147483648.0\n",
    "    elif sampwidth == 1:\n",
    "        arr = np.frombuffer(raw, dtype=np.uint8).astype(np.float32)\n",
    "        arr = (arr - 128.0) / 128.0\n",
    "        return arr if n_channels == 1 else arr.reshape(-1, n_channels).mean(axis=1).astype(np.float32)\n",
    "    else:\n",
    "        dtype = np.int16\n",
    "        scale = 32768.0\n",
    "\n",
    "    arr = np.frombuffer(raw, dtype=dtype).astype(np.float32) / scale\n",
    "    return arr if n_channels == 1 else arr.reshape(-1, n_channels).mean(axis=1).astype(np.float32)\n",
    "\n",
    "\n",
    "def record_until_enter(sr: int = target_sample_rate) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Records microphone audio from default input device using FFmpeg.\n",
    "    - Press Enter to start, press Enter again to stop.\n",
    "    Returns mono float32 numpy array at the given sampling rate.\n",
    "    \"\"\"\n",
    "    if not _has_ffmpeg():\n",
    "        print(\"FFmpeg not found on PATH. Please install FFmpeg or provide a prerecorded WAV file.\")\n",
    "        return np.array([], dtype=np.float32)\n",
    "\n",
    "    print(\"Press Enter to START recording...\")\n",
    "    input()\n",
    "    print(\"Recording... Press Enter to STOP.\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        out_wav = os.path.join(td, \"capture.wav\")\n",
    "        ffmpeg_cmd = [\"ffmpeg\", \"-hide_banner\", \"-loglevel\", \"error\"]\n",
    "        ffmpeg_cmd += _ffmpeg_input_args(sr)\n",
    "        ffmpeg_cmd += [\"-acodec\", \"pcm_s16le\", \"-y\", out_wav]\n",
    "\n",
    "        proc = subprocess.Popen(ffmpeg_cmd, stdin=subprocess.PIPE, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        try:\n",
    "            stopper = threading.Thread(target=lambda: (input()), daemon=True)\n",
    "            stopper.start()\n",
    "            while stopper.is_alive() and proc.poll() is None:\n",
    "                time.sleep(0.05)\n",
    "            if proc.poll() is None and proc.stdin:\n",
    "                try:\n",
    "                    proc.stdin.write(b\"q\")\n",
    "                    proc.stdin.flush()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            try:\n",
    "                proc.wait(timeout=3)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                proc.terminate()\n",
    "        finally:\n",
    "            if proc.poll() is None:\n",
    "                proc.kill()\n",
    "\n",
    "        if not os.path.exists(out_wav) or os.path.getsize(out_wav) == 0:\n",
    "            return np.array([], dtype=np.float32)\n",
    "\n",
    "        return _read_wav_to_numpy(out_wav).astype(np.float32)\n",
    "\n",
    "\n",
    "# Run interactive capture -> preprocess -> infer\n",
    "with torch.inference_mode():\n",
    "    try:\n",
    "        rec_sr = int(target_sample_rate)\n",
    "        audio = record_until_enter(sr=rec_sr)\n",
    "\n",
    "        if audio.size == 0:\n",
    "            print(\"No audio captured.\")\n",
    "        else:\n",
    "            wf = torch.from_numpy(audio)\n",
    "            feats = waveform_to_logmel(wf, rec_sr)  # [1, n_mels, T]\n",
    "            feats = feats.unsqueeze(0).to(device)  # [B=1, 1, n_mels, T]\n",
    "\n",
    "            logits = model(feats)\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
    "            pred_id = int(np.argmax(probs))\n",
    "            pred_label = label_names[pred_id] if pred_id < len(label_names) else str(pred_id)\n",
    "\n",
    "            print(f\"Prediction: {pred_label}  (probs={np.round(probs, 3)})\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted by user.\")\n"
   ],
   "id": "c7fcda28d44846b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Enter to START recording...\n",
      "Recording... Press Enter to STOP.\n",
      "Prediction: 2  (probs=[0.001 0.    0.315 0.258 0.057 0.    0.307 0.    0.06  0.   ])\n"
     ]
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
