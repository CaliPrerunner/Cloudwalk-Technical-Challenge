{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Coudwalk Technical Challenge\n",
    "\n",
    "## Objective:\n",
    "Your task is to build a lightweight prototype that listens to spoken digits (0â€“9) and predicts the correct number. The goal is to find the lightest effective solution. Live microphone input to test your model in real time. This helps explore real-world performance, including latency, noise handling, and usability under less controlled conditions."
   ],
   "id": "c0e5800d150b07d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data loading & Preprocessing",
   "id": "45fd2f19f6dce336"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:01:32.363856Z",
     "start_time": "2025-08-14T19:01:30.613673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data loading & Preprocessing for DS-CNN on Free Spoken Digit Dataset (HF: mteb/free-spoken-digit-dataset)\n",
    "# - Loads dataset and derives label names directly from HF features\n",
    "# - Converts audio to Log-Mel spectrograms suitable for DS-CNN: shape [1, n_mels, T]\n",
    "# - Provides modular utilities and a robust collate function\n",
    "# - Exposes: raw (DatasetDict), label_names, id2label, label2id, mapped_splits (DatasetDict), collate\n",
    "\n",
    "# Ensure required packages\n",
    "try:\n",
    "    from datasets import load_dataset, Audio, DatasetDict\n",
    "except ModuleNotFoundError:\n",
    "    !pip install datasets[audio]\n",
    "    from datasets import load_dataset, Audio, DatasetDict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Torchaudio for spectrograms\n",
    "try:\n",
    "    import torchaudio\n",
    "    from torchaudio.transforms import MelSpectrogram\n",
    "    from torchaudio.functional import amplitude_to_DB\n",
    "except ModuleNotFoundError:\n",
    "    !pip install torchaudio\n",
    "    import torchaudio\n",
    "    from torchaudio.transforms import MelSpectrogram\n",
    "    from torchaudio.functional import amplitude_to_DB\n",
    "\n",
    "\n",
    "def get_or_default(name: str, default):\n",
    "    \"\"\"Fetch a global variable by name if it exists; otherwise return default.\"\"\"\n",
    "    return globals()[name] if name in globals() else default\n",
    "\n",
    "\n",
    "# Configuration (use existing globals if already defined elsewhere in the notebook)\n",
    "target_sample_rate: int = get_or_default(\"target_sample_rate\", 16000)\n",
    "n_mels: int = get_or_default(\"n_mels\", 40)\n",
    "n_fft_val: int = get_or_default(\"n_fft_val\", 512)\n",
    "win_length: int = get_or_default(\"win_length\", int(0.025 * target_sample_rate))  # ~25ms\n",
    "hop_length: int = get_or_default(\"hop_length\", int(0.010 * target_sample_rate))  # ~10ms\n",
    "batch_size: int = get_or_default(\"batch_size\", 128)\n",
    "num_workers: int = get_or_default(\"num_workers\", 2)\n",
    "\n",
    "# 1) Load dataset and derive label names (no hardcoding)\n",
    "raw: DatasetDict = load_dataset(\"mteb/free-spoken-digit-dataset\")\n",
    "label_names = raw[\"train\"].features[\"label\"].names\n",
    "id2label = {i: name for i, name in enumerate(label_names)}\n",
    "label2id = {name: i for i, name in id2label.items()}\n",
    "\n",
    "# Cast/Decode audio at target sampling rate\n",
    "raw = raw.cast_column(\"audio\", Audio(sampling_rate=target_sample_rate))\n",
    "\n",
    "# 2) Preprocess: audio -> Log-Mel spectrogram\n",
    "# Reuse existing mel transform if available, else create a new one\n",
    "if \"melspec\" in globals() and isinstance(globals()[\"melspec\"], MelSpectrogram):\n",
    "    melspec: MelSpectrogram = globals()[\"melspec\"]\n",
    "else:\n",
    "    melspec = MelSpectrogram(\n",
    "        sample_rate=target_sample_rate,\n",
    "        n_fft=n_fft_val,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        f_min=0.0,\n",
    "        f_max=target_sample_rate // 2,\n",
    "        n_mels=n_mels,\n",
    "        center=True,\n",
    "        power=2.0,  # power spectrogram\n",
    "        norm=\"slaney\",\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "\n",
    "def waveform_to_logmel(waveform: torch.Tensor, sample_rate: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert mono waveform [T] or [1, T] to Log-Mel spectrogram [1, n_mels, time].\n",
    "    \"\"\"\n",
    "    if waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)  # [1, T]\n",
    "    # Ensure target sample rate; dataset cast handles this, but keep signature consistent\n",
    "    with torch.no_grad():\n",
    "        mel = melspec(waveform)  # [1, n_mels, time]\n",
    "        # Convert to log scale (dB). Add small offset to avoid log(0).\n",
    "        log_mel = amplitude_to_DB(mel.clamp_min(1e-10), multiplier=10.0, amin=1e-10, db_multiplier=0.0)\n",
    "        # Optional per-utterance normalization (stabilizes training for small models)\n",
    "        mean = log_mel.mean(dim=(-1, -2), keepdim=True)\n",
    "        std = log_mel.std(dim=(-1, -2), keepdim=True).clamp_min(1e-5)\n",
    "        log_mel = (log_mel - mean) / std\n",
    "    return log_mel  # [1, n_mels, time]\n",
    "\n",
    "\n",
    "def _map_example_to_features(batch):\n",
    "    \"\"\"\n",
    "    HF map function: takes an example with keys {\"audio\": {\"array\", \"sampling_rate\"}, \"label\"}\n",
    "    Returns dict with \"input_values\": float32 tensor-like [1, n_mels, time] and \"label\": int\n",
    "    \"\"\"\n",
    "    arr = batch[\"audio\"][\"array\"]\n",
    "    sr = batch[\"audio\"][\"sampling_rate\"]\n",
    "    # Convert to torch waveform\n",
    "    wf = torch.tensor(arr, dtype=torch.float32)\n",
    "    # Compute log-mel\n",
    "    features = waveform_to_logmel(wf, sr)\n",
    "    # Store as list to be HF-serializable; collate will convert back to tensor\n",
    "    return {\n",
    "        \"input_values\": features.squeeze(0).numpy()[None, ...].astype(np.float32),  # [1, n_mels, T] as numpy\n",
    "        \"label\": int(batch[\"label\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply mapping to all splits\n",
    "mapped_splits: DatasetDict = raw.map(\n",
    "    _map_example_to_features,\n",
    "    remove_columns=[c for c in raw[\"train\"].column_names if c not in (\"label\",)],\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Collate for DS-CNN: pads along time dimension to max(T) in batch; output [B, 1, n_mels, T]\n",
    "def collate(batch):\n",
    "    \"\"\"\n",
    "    Collate function:\n",
    "    - Accepts items with {\"input_values\": [1, n_mels, T], \"label\": int}\n",
    "    - Right-pads time dimension to the max length in batch\n",
    "    - Returns (features [B, 1, n_mels, T], labels [B])\n",
    "    \"\"\"\n",
    "    feats, labels = [], []\n",
    "    time_dim = -1\n",
    "    for item in batch:\n",
    "        x = item[\"input_values\"]\n",
    "        # Ensure torch tensor [1, n_mels, T]\n",
    "        if isinstance(x, list):\n",
    "            x = np.array(x, dtype=np.float32)\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)  # [1, n_mels, T]\n",
    "        feats.append(x)\n",
    "        labels.append(int(item[\"label\"]))\n",
    "    # Pad along time\n",
    "    max_T = max(x.shape[time_dim] for x in feats)\n",
    "    padded = []\n",
    "    for x in feats:\n",
    "        t = x.shape[time_dim]\n",
    "        if t < max_T:\n",
    "            pad_shape = list(x.shape)\n",
    "            pad_shape[time_dim] = max_T - t\n",
    "            pad = torch.zeros(pad_shape, dtype=x.dtype)\n",
    "            x = torch.cat([x, pad], dim=time_dim)\n",
    "        elif t > max_T:\n",
    "            idx = [slice(None)] * x.dim()\n",
    "            idx[time_dim] = slice(0, max_T)\n",
    "            x = x[tuple(idx)]\n",
    "        padded.append(x)\n",
    "    features = torch.stack(padded, dim=0)  # [B, 1, n_mels, T]\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Expose commonly used globals for downstream cells\n",
    "globals().update({\n",
    "    \"raw\": raw,\n",
    "    \"label_names\": label_names,\n",
    "    \"id2label\": id2label,\n",
    "    \"label2id\": label2id,\n",
    "    \"mapped_splits\": mapped_splits,\n",
    "    \"collate\": collate,\n",
    "    \"target_sample_rate\": target_sample_rate,\n",
    "    \"n_mels\": n_mels,\n",
    "    \"n_fft_val\": n_fft_val,\n",
    "    \"win_length\": win_length,\n",
    "    \"hop_length\": hop_length,\n",
    "    \"melspec\": melspec,\n",
    "})\n"
   ],
   "id": "d349c84de6388655",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DS-CNN Model & Training",
   "id": "192cf4c22945b829"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:01:32.397315Z",
     "start_time": "2025-08-14T19:01:32.377197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class DSConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise Separable Convolution Block:\n",
    "    - Depthwise 2D convolution (groups=in_channels)\n",
    "    - BatchNorm + ReLU\n",
    "    - Pointwise 1x1 convolution\n",
    "    - BatchNorm + ReLU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: tuple[int, int] = (1, 1),\n",
    "                 dilation: int = 1):\n",
    "        super().__init__()\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                   dilation=dilation, groups=in_channels, bias=False)\n",
    "        self.dw_bn = nn.BatchNorm2d(in_channels)\n",
    "        self.dw_relu = nn.ReLU(inplace=True)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.pw_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.pw_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.depthwise(x)\n",
    "        x = self.dw_bn(x)\n",
    "        x = self.dw_relu(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.pw_bn(x)\n",
    "        x = self.pw_relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DSCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise Separable CNN for keyword spotting / digit recognition.\n",
    "    Expects input of shape [B, 1, n_mels(=40), T].\n",
    "    Architecture:\n",
    "      - Stem conv\n",
    "      - Stack of DSConv blocks with occasional stride for downsampling\n",
    "      - Global average pooling\n",
    "      - Linear classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_mels: int = 40, n_classes: int = 10, channels: tuple[int, ...] = (64, 64, 128, 128, 256)):\n",
    "        super().__init__()\n",
    "        c1, c2, c3, c4, c5 = channels\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.features = nn.Sequential(\n",
    "            DSConvBlock(c1, c2, kernel_size=3, stride=(2, 2)),  # downsample both axes\n",
    "            DSConvBlock(c2, c3, kernel_size=3, stride=(1, 2)),  # further reduce time\n",
    "            DSConvBlock(c3, c4, kernel_size=3, stride=(2, 1)),  # reduce mel\n",
    "            DSConvBlock(c4, c5, kernel_size=3, stride=(1, 1)),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(c5, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.stem(x)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "id": "d4716b53a26e6a48",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:01:32.497811Z",
     "start_time": "2025-08-14T19:01:32.427802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "def _build_splits_from_prepared(prepared: dict, val_size: float = 0.1) -> dict:\n",
    "    \"\"\"\n",
    "    Create train/validation/test splits from prepared_splits.\n",
    "    If validation not present, split from train with stratification.\n",
    "    \"\"\"\n",
    "    splits = dict(prepared)\n",
    "    if \"validation\" not in splits:\n",
    "        tv = splits[\"train\"].train_test_split(test_size=val_size, stratify_by_column=\"label\")\n",
    "        splits[\"train\"], splits[\"validation\"] = tv[\"train\"], tv[\"test\"]\n",
    "    return splits\n",
    "\n",
    "\n",
    "def _num_classes_from_dataset(ds) -> int:\n",
    "    labs = list(set(ds[\"label\"]))\n",
    "    return int(max(labs) + 1)\n",
    "\n",
    "\n",
    "def build_dataloaders(splits: dict, collate_fn, batch_size: int = 128, num_workers: int = 2) -> tuple[\n",
    "    DataLoader, DataLoader, DataLoader]:\n",
    "    train_loader = DataLoader(splits[\"train\"], batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                              collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(splits[\"validation\"], batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                            collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(splits[\"test\"], batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                             collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "prepared_with_val = _build_splits_from_prepared(mapped_splits, val_size=0.1)\n",
    "n_classes = _num_classes_from_dataset(prepared_with_val[\"train\"])\n",
    "train_loader, val_loader, test_loader = build_dataloaders(prepared_with_val, collate, batch_size=128,\n",
    "                                                          num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DSCNN(n_mels=40, n_classes=n_classes).to(device)\n",
    "param_count = count_parameters(model)\n",
    "\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_ckpt_path = models_dir / \"dscnn_best.pt\"\n"
   ],
   "id": "6d69701fc9cc4bf5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Loop",
   "id": "c8aef2575273a35a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:01:32.559680Z",
     "start_time": "2025-08-14T19:01:32.509458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from typing import Dict, List, Tuple, Callable, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _macro_micro_from_confusion(conf_mat: torch.Tensor) -> Dict[str, float]:\n",
    "    # conf_mat shape: [C, C] where rows = true, cols = pred\n",
    "    cm = conf_mat.float()\n",
    "    tp = torch.diag(cm)\n",
    "    fp = cm.sum(0) - tp\n",
    "    fn = cm.sum(1) - tp\n",
    "    tn = cm.sum() - (tp + fp + fn)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    eps = 1e-12\n",
    "\n",
    "    precision_per_class = tp / torch.clamp(tp + fp, min=eps)\n",
    "    recall_per_class = tp / torch.clamp(tp + fn, min=eps)\n",
    "    f1_per_class = 2 * precision_per_class * recall_per_class / torch.clamp(precision_per_class + recall_per_class,\n",
    "                                                                            min=eps)\n",
    "\n",
    "    macro_precision = precision_per_class.mean().item()\n",
    "    macro_recall = recall_per_class.mean().item()\n",
    "    macro_f1 = f1_per_class.mean().item()\n",
    "\n",
    "    # Micro = compute from totals\n",
    "    tp_sum = tp.sum()\n",
    "    fp_sum = fp.sum()\n",
    "    fn_sum = fn.sum()\n",
    "    micro_precision = (tp_sum / torch.clamp(tp_sum + fp_sum, min=eps)).item()\n",
    "    micro_recall = (tp_sum / torch.clamp(tp_sum + fn_sum, min=eps)).item()\n",
    "    micro_f1 = (2 * micro_precision * micro_recall / max(1e-12, (micro_precision + micro_recall))) if (\n",
    "                                                                                                              micro_precision + micro_recall) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision_macro\": macro_precision,\n",
    "        \"recall_macro\": macro_recall,\n",
    "        \"f1_macro\": macro_f1,\n",
    "        \"precision_micro\": micro_precision,\n",
    "        \"recall_micro\": micro_recall,\n",
    "        \"f1_micro\": micro_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_confusion(model: nn.Module, loader: DataLoader, device: torch.device, n_classes: int) -> Dict[\n",
    "    str, float]:\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_sum = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    conf_mat = torch.zeros((n_classes, n_classes), dtype=torch.long, device=device)\n",
    "\n",
    "    for feats, labels in loader:\n",
    "        feats = feats.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = model(feats)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "\n",
    "        # Update confusion matrix\n",
    "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "            conf_mat[t.long(), p.long()] += 1\n",
    "\n",
    "    metrics = _macro_micro_from_confusion(conf_mat)\n",
    "    metrics.update({\n",
    "        \"loss\": loss_sum / max(1, total),\n",
    "        \"acc\": correct / max(1, total),\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_one_epoch_with_metrics(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer,\n",
    "                                 device: torch.device, n_classes: int,\n",
    "                                 scaler: Optional[torch.cuda.amp.GradScaler] = None) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    correct = 0\n",
    "    conf_mat = torch.zeros((n_classes, n_classes), dtype=torch.long, device=device)\n",
    "\n",
    "    # Gradient clipping config\n",
    "    max_norm = 1.0  # L2 norm clipping\n",
    "\n",
    "    for b_idx, (feats, labels) in enumerate(loader):\n",
    "        feats = feats.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(feats)\n",
    "                loss = criterion(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Unscale then clip before stepping\n",
    "            scaler.unscale_(optimizer)\n",
    "            global_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(feats)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip before optimizer.step()\n",
    "            global_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "            conf_mat[t.long(), p.long()] += 1\n",
    "\n",
    "    base = {\n",
    "        \"loss\": loss_sum / max(1, total),\n",
    "        \"acc\": correct / max(1, total),\n",
    "    }\n",
    "    base.update(_macro_micro_from_confusion(conf_mat))\n",
    "    return base\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_model_only_latency(model: nn.Module, loader: DataLoader, device: torch.device,\n",
    "                               warmup_batches: int = 5, max_batches: Optional[int] = None) -> Dict[str, float]:\n",
    "    # Measures forward-pass latency from already-precomputed features to logits.\n",
    "    model.eval()\n",
    "    times: List[float] = []\n",
    "    seen = 0\n",
    "\n",
    "    # Warmup\n",
    "    it = iter(loader)\n",
    "    for _ in range(warmup_batches):\n",
    "        try:\n",
    "            feats, _ = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        _ = model(feats.to(device))\n",
    "\n",
    "    # Timed runs\n",
    "    it = iter(loader)\n",
    "    with torch.inference_mode():\n",
    "        for b_idx, (feats, _) in enumerate(it):\n",
    "            if max_batches is not None and b_idx >= max_batches:\n",
    "                break\n",
    "            feats = feats.to(device)\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            t0 = time.perf_counter()\n",
    "            _ = model(feats)\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            t1 = time.perf_counter()\n",
    "            batch_time = t1 - t0\n",
    "            per_sample = batch_time / max(1, feats.size(0))\n",
    "            times.append(per_sample)\n",
    "            seen += feats.size(0)\n",
    "\n",
    "    if len(times) == 0:\n",
    "        return {\"latency_mean_s\": float(\"nan\"), \"latency_p50_s\": float(\"nan\"), \"latency_p95_s\": float(\"nan\")}\n",
    "    arr = np.array(times)\n",
    "    return {\n",
    "        \"latency_mean_s\": float(arr.mean()),\n",
    "        \"latency_p50_s\": float(np.percentile(arr, 50)),\n",
    "        \"latency_p95_s\": float(np.percentile(arr, 95)),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_end_to_end_latency(\n",
    "        model: nn.Module,\n",
    "        dataset,\n",
    "        device: torch.device,\n",
    "        featurize_fn: Callable[[torch.Tensor, int], torch.Tensor],\n",
    "        sample_rate_getter: Optional[Callable[[int], int]] = None,\n",
    "        num_samples: int = 100,\n",
    "        warmup: int = 10,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Measures end-to-end latency from raw audio -> features -> classification.\n",
    "\n",
    "    Arguments:\n",
    "      - dataset: indexable dataset where dataset[i] returns either (waveform, label) or (waveform, sample_rate, label)\n",
    "      - featurize_fn: function that maps (waveform, sample_rate) -> features tensor shaped as model expects (C, T, ...) or (F, T)\n",
    "      - sample_rate_getter: optional function to get sample rate when dataset[i] does not return it; called as sample_rate_getter(i)\n",
    "      - num_samples: number of random samples from dataset to time\n",
    "      - warmup: number of warmup runs to exclude from timing\n",
    "\n",
    "    Returns dict with mean/p50/p95 in seconds per sample.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rng = np.random.default_rng(0)\n",
    "    indices = rng.choice(len(dataset), size=min(num_samples + warmup, len(dataset)), replace=False)\n",
    "    times: List[float] = []\n",
    "\n",
    "    # Helper to extract (waveform, sr) regardless of dataset format\n",
    "    def _get_waveform_sr(idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        item = dataset[idx]\n",
    "        if isinstance(item, (tuple, list)):\n",
    "            if len(item) == 3:\n",
    "                waveform, sample_rate, _ = item\n",
    "            elif len(item) == 2:\n",
    "                waveform, _ = item\n",
    "                sample_rate = sample_rate_getter(idx) if sample_rate_getter is not None else 16000\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported dataset item format.\")\n",
    "        else:\n",
    "            raise ValueError(\"Dataset item must be tuple/list.\")\n",
    "        return waveform, int(sample_rate)\n",
    "\n",
    "    # Warmup\n",
    "    for i in range(min(warmup, len(indices))):\n",
    "        wf, sr = _get_waveform_sr(indices[i])\n",
    "        feats = featurize_fn(wf, sr)\n",
    "        feats = feats.unsqueeze(0).to(device) if feats.dim() == 3 else feats.to(device).unsqueeze(0)\n",
    "        _ = model(feats)\n",
    "\n",
    "    # Timed runs\n",
    "    with torch.inference_mode():\n",
    "        for i in range(warmup, len(indices)):\n",
    "            wf, sr = _get_waveform_sr(indices[i])\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            t0 = time.perf_counter()\n",
    "            feats = featurize_fn(wf, sr)\n",
    "            feats = feats.unsqueeze(0).to(device) if feats.dim() == 3 else feats.to(device).unsqueeze(0)\n",
    "            _ = model(feats)\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            t1 = time.perf_counter()\n",
    "            times.append(t1 - t0)\n",
    "\n",
    "    if len(times) == 0:\n",
    "        return {\"latency_mean_s\": float(\"nan\"), \"latency_p50_s\": float(\"nan\"), \"latency_p95_s\": float(\"nan\")}\n",
    "    arr = np.array(times)\n",
    "    return {\n",
    "        \"latency_mean_s\": float(arr.mean()),\n",
    "        \"latency_p50_s\": float(np.percentile(arr, 50)),\n",
    "        \"latency_p95_s\": float(np.percentile(arr, 95)),\n",
    "        \"samples_timed\": int(len(arr)),\n",
    "    }\n"
   ],
   "id": "5511742d82087992",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:02:31.777030Z",
     "start_time": "2025-08-14T19:01:32.582518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assumes you already have:\n",
    "# - model, train_loader, val_loader, test_loader, device, n_classes, best_ckpt_path\n",
    "# - epochs, optimizer, scheduler, scaler (optional)\n",
    "# If you previously defined train_one_epoch/evaluate, we now use the enhanced versions below.\n",
    "\n",
    "# Robust collate to handle cases where \"input_values\" may be a list instead of a Tensor\n",
    "def _safe_collate(batch):\n",
    "    import torch\n",
    "    feats = []\n",
    "    labels = []\n",
    "    time_dim = -1\n",
    "    for b in batch:\n",
    "        x = b[\"input_values\"]\n",
    "        if isinstance(x, list):\n",
    "            # unwrap single-element lists\n",
    "            x = x[0] if len(x) > 0 else x\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x)\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)  # [1, n_mels, T]\n",
    "        feats.append(x)\n",
    "        labels.append(int(b[\"label\"]))\n",
    "    # pad/truncate to max time in batch\n",
    "    max_time = max(x.shape[time_dim] for x in feats)\n",
    "    padded = []\n",
    "    for x in feats:\n",
    "        t = x.shape[time_dim]\n",
    "        if t > max_time:\n",
    "            idx = [slice(None)] * x.dim()\n",
    "            idx[time_dim] = slice(0, max_time)\n",
    "            x = x[tuple(idx)]\n",
    "        elif t < max_time:\n",
    "            pad_shape = list(x.shape)\n",
    "            pad_shape[time_dim] = max_time - t\n",
    "            pad_tensor = torch.zeros(pad_shape, dtype=x.dtype)\n",
    "            x = torch.cat([x, pad_tensor], dim=time_dim)\n",
    "        padded.append(x)\n",
    "    batch_features = torch.stack(padded, dim=0)  # [B, 1, n_mels, time]\n",
    "    batch_labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "\n",
    "# Rebuild DataLoaders with the robust collate to avoid shape/list issues\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    prepared_with_val[\"train\"],\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=_safe_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    prepared_with_val[\"validation\"],\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=_safe_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    prepared_with_val[\"test\"],\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=_safe_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "\n",
    "# Clear session and CUDA cache between runs, then re-instantiate everything fresh\n",
    "def clear_session():\n",
    "    import gc\n",
    "    import torch\n",
    "    # Drop references if they exist\n",
    "    for name in (\"model\", \"optimizer\", \"scheduler\", \"scaler\"):\n",
    "        if name in globals():\n",
    "            try:\n",
    "                del globals()[name]\n",
    "            except Exception:\n",
    "                pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "clear_session()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Fresh instantiation for each run\n",
    "epochs = 25\n",
    "lr = 0.002\n",
    "\n",
    "model = DSCNN(n_mels=40, n_classes=n_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "best_val_acc = 0.0\n",
    "history = {\n",
    "    \"train_loss\": [], \"train_acc\": [],\n",
    "    \"train_precision_macro\": [], \"train_recall_macro\": [], \"train_f1_macro\": [],\n",
    "    \"val_loss\": [], \"val_acc\": [],\n",
    "    \"val_precision_macro\": [], \"val_recall_macro\": [], \"val_f1_macro\": [],\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()  # ensure training mode each epoch\n",
    "    train_metrics = train_one_epoch_with_metrics(model, train_loader, optimizer, device, n_classes, scaler)\n",
    "    val_metrics = evaluate_with_confusion(model, val_loader, device, n_classes)\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "    history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
    "    history[\"train_precision_macro\"].append(train_metrics[\"precision_macro\"])\n",
    "    history[\"train_recall_macro\"].append(train_metrics[\"recall_macro\"])\n",
    "    history[\"train_f1_macro\"].append(train_metrics[\"f1_macro\"])\n",
    "\n",
    "    history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "    history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
    "    history[\"val_precision_macro\"].append(val_metrics[\"precision_macro\"])\n",
    "    history[\"val_recall_macro\"].append(val_metrics[\"recall_macro\"])\n",
    "    history[\"val_f1_macro\"].append(val_metrics[\"f1_macro\"])\n",
    "\n",
    "    if val_metrics[\"acc\"] > best_val_acc:\n",
    "        best_val_acc = val_metrics[\"acc\"]\n",
    "        torch.save(\n",
    "            {\"model_state\": model.state_dict(), \"config\": {\"n_classes\": n_classes}},\n",
    "            best_ckpt_path\n",
    "        )\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train: loss={train_metrics['loss']:.4f}, acc={train_metrics['acc']:.4f}, f1={train_metrics['f1_macro']:.4f} | \"\n",
    "          f\"Val: loss={val_metrics['loss']:.4f}, acc={val_metrics['acc']:.4f}, f1={val_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "best_val_acc\n"
   ],
   "id": "19683b84b452fa6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[grad clip] step 0: global_norm=0.6533, max_norm=1.0\n",
      "Epoch 01 | Train: loss=2.1720, acc=0.2070, f1=0.1750 | Val: loss=2.3704, acc=0.1000, f1=0.0182\n",
      "[grad clip] step 0: global_norm=0.8746, max_norm=1.0\n",
      "Epoch 02 | Train: loss=1.7603, acc=0.4457, f1=0.4239 | Val: loss=3.7764, acc=0.1074, f1=0.0297\n",
      "[grad clip] step 0: global_norm=1.1774, max_norm=1.0\n",
      "Epoch 03 | Train: loss=1.2681, acc=0.7342, f1=0.7246 | Val: loss=1.3514, acc=0.5074, f1=0.5013\n",
      "[grad clip] step 0: global_norm=0.9693, max_norm=1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 115\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    114\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# ensure training mode each epoch\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m     train_metrics \u001B[38;5;241m=\u001B[39m train_one_epoch_with_metrics(model, train_loader, optimizer, device, n_classes, scaler)\n\u001B[1;32m    116\u001B[0m     val_metrics \u001B[38;5;241m=\u001B[39m evaluate_with_confusion(model, val_loader, device, n_classes)\n\u001B[1;32m    117\u001B[0m     scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "Cell \u001B[0;32mIn[22], line 120\u001B[0m, in \u001B[0;36mtrain_one_epoch_with_metrics\u001B[0;34m(model, loader, optimizer, device, n_classes, scaler)\u001B[0m\n\u001B[1;32m    118\u001B[0m     scaler\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 120\u001B[0m     logits \u001B[38;5;241m=\u001B[39m model(feats)\n\u001B[1;32m    121\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(logits, labels)\n\u001B[1;32m    122\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[20], line 69\u001B[0m, in \u001B[0;36mDSCNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m     68\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstem(x)\n\u001B[0;32m---> 69\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures(x)\n\u001B[1;32m     70\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(x)\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     71\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(x)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[20], line 30\u001B[0m, in \u001B[0;36mDSConvBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m---> 30\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdepthwise(x)\n\u001B[1;32m     31\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdw_bn(x)\n\u001B[1;32m     32\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdw_relu(x)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    553\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 554\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[1;32m    539\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[1;32m    540\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    547\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[1;32m    548\u001B[0m     )\n\u001B[0;32m--> 549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[1;32m    550\u001B[0m     \u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups\n\u001B[1;32m    551\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Graphs\n",
   "id": "294802c335958cff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:02:31.920951320Z",
     "start_time": "2025-08-14T18:16:07.932013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(\"Plot results\", exist_ok=True)\n",
    "\n",
    "epochs_range = range(1, len(history[\"train_acc\"]) + 1)\n",
    "\n",
    "# 1) Final test metrics\n",
    "with torch.no_grad():\n",
    "    test_metrics = evaluate_with_confusion(model, test_loader, device, n_classes)\n",
    "\n",
    "# 2) Model-only latency (forward pass on precomputed features)\n",
    "#    Limit batches to keep timing quick; adjust as needed.\n",
    "model_latency = measure_model_only_latency(\n",
    "    model, val_loader, device, warmup_batches=5, max_batches=20\n",
    ")\n",
    "\n",
    "# 3) End-to-end latency (raw audio -> features -> model).\n",
    "#    If you have a raw-audio dataset and a featurize_fn available, compute it here.\n",
    "#    For now, set to None so plotting code can skip it gracefully.\n",
    "end_to_end_latency = None\n",
    "\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "print(\"Model-only latency:\", model_latency)\n",
    "\n",
    "# 1) Accuracy curves (own graph)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_range, history[\"train_acc\"], label=\"Train Acc\", marker=\"o\", linewidth=2)\n",
    "plt.plot(epochs_range, history[\"val_acc\"], label=\"Val Acc\", marker=\"s\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Accuracy over epochs\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"Plot results\", \"accuracy_over_epochs.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 2) Precision (macro) curves (own graph)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_range, history[\"train_precision_macro\"], label=\"Train Precision (macro)\", marker=\"o\", linewidth=2)\n",
    "plt.plot(epochs_range, history[\"val_precision_macro\"], label=\"Val Precision (macro)\", marker=\"s\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Precision (macro)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Precision (macro) over epochs\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"Plot results\", \"precision_macro_over_epochs.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 3) F1 (macro) curves (own graph)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_range, history[\"train_f1_macro\"], label=\"Train F1 (macro)\", marker=\"o\", linewidth=2)\n",
    "plt.plot(epochs_range, history[\"val_f1_macro\"], label=\"Val F1 (macro)\", marker=\"s\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 (macro)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"F1 (macro) over epochs\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"Plot results\", \"f1_macro_over_epochs.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 4) Final test set metrics bar chart\n",
    "final_names = [\"Accuracy\", \"Precision (macro)\", \"Recall (macro)\", \"F1 (macro)\"]\n",
    "final_vals = [\n",
    "    test_metrics.get(\"acc\", float(\"nan\")),\n",
    "    test_metrics.get(\"precision_macro\", float(\"nan\")),\n",
    "    test_metrics.get(\"recall_macro\", float(\"nan\")),\n",
    "    test_metrics.get(\"f1_macro\", float(\"nan\")),\n",
    "]\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(final_names, final_vals, color=[\"#4caf50\", \"#2196f3\", \"#ff9800\", \"#9c27b0\"])\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Final Test Metrics\")\n",
    "for b, v in zip(bars, final_vals):\n",
    "    plt.text(b.get_x() + b.get_width() / 2, b.get_height() + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"Plot results\", \"final_test_metrics.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# 5) Latency plot (own graph; model-only; end-to-end if available)\n",
    "# We plot horizontal bars for summary stats in milliseconds for readability\n",
    "def _plot_latency_summary(lat_dict: Dict[str, float], title: str, filename: str):\n",
    "    if lat_dict is None:\n",
    "        return\n",
    "    keys = [k for k in [\"latency_mean_s\", \"latency_p50_s\", \"latency_p95_s\"] if k in lat_dict]\n",
    "    vals_ms = [lat_dict[k] * 1000.0 for k in keys]\n",
    "    labels = [\"Mean (ms)\", \"P50 (ms)\", \"P95 (ms)\"]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bars = plt.barh(labels, vals_ms, color=\"#607d8b\")\n",
    "    plt.xlabel(\"Milliseconds per sample\")\n",
    "    plt.title(title)\n",
    "    for b, v in zip(bars, vals_ms):\n",
    "        plt.text(v, b.get_y() + b.get_height() / 2, f\" {v:.2f} ms\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"Plot results\", filename), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "_plot_latency_summary(model_latency, \"Model-only Latency Summary\", \"latency_model_only.png\")\n",
    "if end_to_end_latency is not None:\n",
    "    _plot_latency_summary(end_to_end_latency, \"End-to-End Latency Summary\", \"latency_end_to_end.png\")\n",
    "\n",
    "print('Saved plots to \"Plot results\" folder.')\n"
   ],
   "id": "96c7a2a443ad8274",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'precision_macro': 0.012240314856171608, 'recall_macro': 0.07999999821186066, 'f1_macro': 0.018703008070588112, 'precision_micro': 0.07999999821186066, 'recall_micro': 0.07999999821186066, 'f1_micro': 0.07999999821186066, 'loss': 2.3238205464680988, 'acc': 0.08}\n",
      "Model-only latency: {'latency_mean_s': 0.0016528858344539035, 'latency_p50_s': 0.0018740283984328698, 'latency_p95_s': 0.001938971782718722}\n",
      "Saved plots to \"Plot results\" folder.\n"
     ]
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
